\clearpage

\section{基于改进DeepLabV3+的OCT图像语义分割方法}

\subsection{引言}

第二章介绍了 OCT 成像原理、深度学习基础以及语义分割的理论基础，为本章的方法设计提供了理论支撑。针对 DeepLabV3+ 在处理 OCT 图像时存在的全局上下文建模不足和局部细节丢失问题，本章提出了一种基于双重注意力机制的改进方案：在编码器端引入 TR（Transformer Routing）模块增强全局依赖建模，在解码器端引入 SAE（Spatial Attention Enhancement）模块增强局部细节表达。

本章结构如下：3.2 节介绍总体框架与改进动机；3.3 节详细介绍 DeepLabV3+ 基线模型；3.4 节和 3.5 节分别阐述 TR 模块和 SAE 模块的设计原理；3.6 节介绍混合损失函数；3.7 节对本章内容进行总结。

\subsection{总体框架}

本文提出的方法基于 DeepLabV3+ 的编码器-解码器架构，通过引入 TR 全局注意力模块和 SAE 空间细节增强模块，实现了对 OCT 图像中锁孔区域的高精度分割。整体网络架构如图 3-1 所示，主要包括编码器、ASPP 模块、TR 模块、解码器和 SAE 模块等核心组件。

\subsubsection{改进动机与模块定位}

DeepLabV3+ 作为语义分割领域的经典方法，在多尺度上下文信息捕获方面表现出色\upcite{Chen2018DeepLab}。然而，在处理激光焊接 OCT 图像时，该模型仍面临两个核心挑战：

\textbf{（1）全局上下文建模不足}：ASPP 模块的感受野受限于空洞卷积的局部性，难以建立长距离依赖关系。OCT 图像中的锁孔区域呈现细长结构，需要理解整个图像的空间关系才能准确分割。

\textbf{（2）局部细节信息丢失}：解码器在特征融合过程中，低层的细节信息可能被高层语义信息淹没。OCT 图像中锁孔边缘对比度低、边界模糊，需要保留更多局部细节实现精确边界定位。

针对上述问题，本文的改进策略如下：在 ASPP 模块之后引入 TR 模块，通过双层路由注意力机制建立长距离依赖；在解码器特征融合之后引入 SAE 模块，通过空间-通道双重注意力增强边界细节。此外，采用 BCE+Dice 混合损失函数应对类别不平衡问题。

\subsubsection{网络架构设计}

本文方法采用编码器-解码器结构，数据流可描述为：输入图像$\rightarrow$编码器$\rightarrow$ASPP$\rightarrow$TR模块$\rightarrow$解码器$\rightarrow$SAE模块$\rightarrow$分类头$\rightarrow$输出。各模块功能如下：

\textbf{（1）编码器}：采用 ResNet18-V1c 作为骨干网络\upcite{He2015ResNet}，通过四阶段下采样提取多尺度特征。输入图像（512×512×3）经编码器后，Stage 4 输出高层语义特征（16×16×512），Stage 1 输出低层细节特征（128×128×64）用于解码器融合。

\textbf{（2）ASPP 模块}：通过 5 个并行分支（1×1 卷积、dilation=12/24/36 的空洞卷积、全局平均池化）捕获多尺度上下文信息\upcite{Chen2017DeepLabV3}，输出特征维度为 16×16×2560。

\textbf{（3）TR 模块}：位于 ASPP 之后，通过双层路由注意力机制增强全局上下文建模。采用窗口划分与 TopK 路由选择，在保持全局建模能力的同时降低计算复杂度。

\textbf{（4）解码器}：将高层特征上采样后与低层特征融合，得到 128×128×560 的融合特征。

\textbf{（5）SAE 模块}：位于解码器融合之后，通过坐标注意力和通道注意力的协同增强，提升边界分割精度。

\textbf{（6）分类头}：通过深度可分离卷积和上采样，输出与输入图像尺寸相同的分割结果（512×512×2）。

\subsection{DeepLabV3+基线模型}

DeepLabV3+ 是语义分割领域的经典方法，通过编码器-解码器结构和 ASPP 模块，在多尺度上下文信息捕获方面表现出色\upcite{Chen2018DeepLab}。本节详细介绍 DeepLabV3+ 的架构设计，包括骨干网络、ASPP 模块和解码器，并分析其在处理 OCT 图像时的局限性，为后续改进模块的设计提供动机。

\subsubsection{编码器-解码器架构}

DeepLabV3+ 采用编码器-解码器架构，编码器负责提取多尺度特征并捕获语义信息，解码器负责恢复空间分辨率并输出像素级预测结果\upcite{Chen2018DeepLab}。编码器通常采用预训练的 CNN 骨干网络（如 ResNet、Xception 等），通过逐层卷积和下采样操作，将输入图像转换为高维特征表示；解码器通过上采样和特征融合操作，逐步恢复特征图的空间分辨率，并输出与输入图像尺寸相同的预测结果。

DeepLabV3+ 的创新之处在于将 DeepLabV3 的输出作为编码器特征，通过解码器融合编码器的中间特征，在保持分割精度的同时提升了边界定位的准确性\upcite{Chen2018DeepLab}。本文方法以 DeepLabV3+ 为基础框架，在此基础上引入 TR 模块和 SAE 模块，进一步提升分割性能。

\subsubsection{骨干网络：ResNet18-V1c}

本文方法采用 ResNet18-V1c 作为编码器的骨干网络\upcite{He2015ResNet}。ResNet18-V1c 是 ResNet 的改进版本，主要特点包括：

\textbf{（1）Deep Stem 结构}：ResNetV1c 使用三个连续的 3×3 卷积替换传统的 7×7 卷积，这种设计能够减少参数量并提升特征提取能力。Deep Stem 结构首先通过三个 3×3 卷积进行特征提取，然后通过最大池化操作进行下采样。

\textbf{（2）残差连接}：ResNet18-V1c 通过残差连接解决了深层网络的梯度消失问题，使得训练更深的网络成为可能\upcite{He2015ResNet}。每个残差块包含两个 3×3 卷积层，通过残差连接将输入直接传递到输出，简化了优化过程。

\textbf{（3）四个阶段的特征提取}：ResNet18-V1c 包含四个阶段（Stage），逐步提取从低级到高级的特征表示。Stage 1 至 Stage 4 的下采样率分别为 1/4、1/8、1/16、1/32，通道数依次为 64、128、256、512。Stage 4 的输出特征（16×16×512）具有丰富的语义信息，被送入 ASPP 模块进行多尺度上下文聚合；Stage 1 的输出特征（128×128×64）保留了较多的细节信息，在解码器中与高层特征进行融合。

\subsubsection{ASPP模块设计}

ASPP（Atrous Spatial Pyramid Pooling）模块是 DeepLabV3+ 的核心组件，通过多个不同空洞率的并行分支捕获多尺度上下文信息\upcite{Chen2017DeepLabV3}。本文方法采用深度可分离 ASPP 模块（DepthwiseSeparableASPPModule），在保持性能的同时降低了计算复杂度。

ASPP 模块包含 5 个并行分支：

\textbf{（1）1×1 卷积分支}：使用标准 1×1 卷积，感受野为 1×1，用于捕获细节特征。该分支能够保留特征图的原始信息，避免空洞卷积可能带来的信息丢失。

\textbf{（2）3×3 空洞卷积分支（dilation=12）}：使用 3×3 空洞卷积，空洞率为 12，感受野约为 25×25，用于捕获局部特征。该分支能够在保持特征图分辨率的同时扩大感受野，捕获更大范围的上下文信息。

\textbf{（3）3×3 空洞卷积分支（dilation=24）}：使用 3×3 空洞卷积，空洞率为 24，感受野约为 49×49，用于捕获区域特征。该分支进一步扩大了感受野，能够捕获更大范围的上下文信息。

\textbf{（4）3×3 空洞卷积分支（dilation=36）}：使用 3×3 空洞卷积，空洞率为 36，感受野约为 73×73，用于捕获全局特征。该分支具有最大的感受野，能够捕获更大范围的上下文信息。

\textbf{（5）全局平均池化分支}：对特征图进行全局平均池化，然后通过 1×1 卷积和双线性插值上采样，恢复原始尺寸。该分支的感受野为整个特征图，能够捕获全局上下文信息。

5 个并行分支的输出特征经过拼接后，得到多尺度特征表示（16×16×2560）。随后通过 1×1 卷积降维到 16×16×512，减少特征维度并融合多尺度信息。

\textbf{深度可分离卷积}：ASPP 模块使用深度可分离卷积降低计算复杂度。深度可分离卷积分为两个步骤：首先进行深度卷积（Depthwise Convolution），对每个通道独立进行卷积操作；然后进行点卷积（Pointwise Convolution），使用 1×1 卷积进行通道融合。深度可分离卷积的参数量和计算量远小于标准卷积，在保持性能的同时提升了计算效率。

\subsubsection{解码器设计}

DeepLabV3+ 的解码器通过上采样和特征融合操作，逐步恢复特征图的空间分辨率\upcite{Chen2018DeepLab}。解码器的设计包括以下几个步骤：

\textbf{（1）高层特征上采样}：ASPP 模块输出的高层特征（16×16×512）经过 1×1 卷积降维后，通过双线性插值上采样到 128×128×512，恢复空间分辨率。

\textbf{（2）低层特征处理}：编码器 Stage 1 的低层特征（128×128×64）通过 1×1 卷积降维到 128×128×48，减少通道数以匹配高层特征的通道数。

\textbf{（3）特征融合}：将上采样后的高层特征（128×128×512）与处理后的低层特征（128×128×48）在通道维度进行拼接，得到融合特征（128×128×560）。这种设计能够结合低层的细节信息和高层的语义信息，提升分割精度。

\textbf{（4）分类头}：融合特征经过深度可分离卷积和 1×1 卷积，转换为类别 logits（128×128×2），最后通过双线性插值上采样到原始图像尺寸（512×512×2），输出像素级类别预测结果。

\subsubsection{基线模型在OCT图像分割中的局限性}

尽管 DeepLabV3+ 在通用语义分割任务中表现出色，但在处理激光焊接 OCT 图像时仍存在以下局限性，这些局限性构成了本文改进工作的直接动机。

\textbf{（1）全局上下文建模不足}：ASPP 模块通过不同空洞率的并行分支捕获多尺度上下文信息，但其感受野仍受限于空洞卷积的局部性。以 dilation=36 的最大空洞率为例，其有效感受野约为 73×73 像素，仅覆盖 16×16 特征图的局部区域。然而，OCT 图像中的锁孔区域往往呈现细长结构（纵横比可达 1:10 以上），跨越图像的多个区域，需要网络理解整个图像的空间关系才能准确分割。ASPP 模块的局部感受野限制了其对全局结构的理解能力，可能导致细长锁孔结构的断裂或误分割。

\textbf{（2）局部细节信息丢失}：解码器通过简单的通道拼接融合高低层特征，但在融合过程中缺乏对特征重要性的自适应调节。低层特征（Stage 1）虽然保留了丰富的边缘和纹理信息，但其通道数（48 维）远小于高层特征（512 维），在特征拼接后容易被高层语义信息所淹没。OCT 图像中锁孔边缘的对比度低（信噪比通常低于常规自然图像）、边界模糊（受散斑噪声影响），需要网络显式增强并保留局部细节信息才能实现精确的边界定位。

\textbf{（3）类别不平衡问题}：OCT 图像中背景区域通常占据 90\% 以上的像素，而锁孔目标区域仅占 5-10\%。标准交叉熵损失对每个像素赋予相同权重，导致模型训练时偏向背景类别，难以充分学习前景特征。

基于上述局限性分析，本文提出针对性的改进方案：在编码器端引入 TR 模块建立长距离依赖关系，在解码器端引入 SAE 模块增强边界细节表达，并设计混合损失函数应对类别不平衡问题。

\subsection{TR全局注意力模块}

针对 DeepLabV3+ 全局上下文建模不足的问题，本文在 ASPP 模块之后引入 TR（Transformer Routing）全局注意力模块。TR 模块借鉴 BiFormer 的双层路由注意力思想\upcite{zhu2023biformervisiontransformerbilevel}，通过区域级路由和 TopK 选择机制，仅对最相关的窗口进行注意力计算，在保持全局建模能力的同时将计算复杂度从 $O(n^2)$ 降低到 $O(n^2 \times k/p^2)$。

\subsubsection{模块结构}

TR 模块主要包括位置编码（深度可分离卷积，3×3）、LayerNorm 归一化、双层路由注意力、MLP（扩展-压缩结构）和残差连接等组件\upcite{zhu2023biformervisiontransformerbilevel}。其中，双层路由注意力是核心创新，通过区域级路由和令牌级注意力两个阶段实现高效的全局上下文建模。

\subsubsection{双层路由注意力机制}

双层路由注意力机制是 TR 模块的核心创新，通过区域级路由和令牌级注意力两个阶段，实现高效的全局上下文建模。具体而言，双层路由注意力机制包括以下步骤：

\textbf{（1）窗口划分}：将输入特征图（16×16×2560）按照 4×4 的窗口大小进行划分，得到 16 个窗口（4×4=16）。每个窗口包含 16 个令牌（token），对应特征图中的 16 个像素位置。

\textbf{（2）区域级路由}：对每个窗口的所有令牌求平均，得到区域级查询向量 $\mathbf{Q}_{\text{region}}^{(i)}$ 和键向量 $\mathbf{K}_{\text{region}}^{(i)}$，其中 $i$ 表示第 $i$ 个窗口。区域级查询和键向量的计算可表示为：
\begin{equation}
\mathbf{Q}_{\text{region}}^{(i)} = \frac{1}{|\mathcal{W}_i|} \sum_{j \in \mathcal{W}_i} \mathbf{Q}_j, \quad \mathbf{K}_{\text{region}}^{(i)} = \frac{1}{|\mathcal{W}_i|} \sum_{j \in \mathcal{W}_i} \mathbf{K}_j
\label{eq:region_qk}
\end{equation}
其中，$\mathcal{W}_i$ 表示第 $i$ 个窗口内的所有令牌，$|\mathcal{W}_i|$ 表示窗口内的令牌数量。

计算窗口间的相似度矩阵 $\mathbf{S}$，其中 $\mathbf{S}_{ij}$ 表示第 $i$ 个窗口与第 $j$ 个窗口的相似度：
\begin{equation}
\mathbf{S}_{ij} = \frac{\mathbf{Q}_{\text{region}}^{(i)} \cdot \mathbf{K}_{\text{region}}^{(j)}}{\|\mathbf{Q}_{\text{region}}^{(i)}\| \times \|\mathbf{K}_{\text{region}}^{(j)}\|}
\label{eq:routing_similarity}
\end{equation}
其中，$\cdot$ 表示向量内积，$\|\cdot\|$ 表示向量范数。相似度矩阵 $\mathbf{S}$ 的维度为 16×16，表示 16 个窗口之间的相似度关系。

\textbf{（3）TopK 路由选择}：对每个窗口，根据相似度矩阵选择 TopK 个最相关的窗口参与注意力计算。本文设置 $k=4$，即每个窗口选择 4 个最相关的窗口。TopK 路由选择可表示为：
\begin{equation}
\mathcal{R}_i = \text{TopK}(\mathbf{S}_{i,:}, k=4)
\label{eq:topk_routing}
\end{equation}
其中，$\mathcal{R}_i$ 表示第 $i$ 个窗口选择的相关窗口索引集合，$\mathbf{S}_{i,:}$ 表示相似度矩阵的第 $i$ 行。

\textbf{（4）令牌级注意力}：在选定的窗口内进行像素级注意力计算。对于第 $i$ 个窗口，仅对 $\mathcal{R}_i$ 中的窗口进行注意力计算，从而降低计算复杂度。令牌级注意力可表示为：
\begin{equation}
\text{Attention}(\mathbf{Q}_i, \mathbf{K}_{\mathcal{R}_i}, \mathbf{V}_{\mathcal{R}_i}) = \text{softmax}\left(\frac{\mathbf{Q}_i \mathbf{K}_{\mathcal{R}_i}^T}{\sqrt{d_k}}\right) \mathbf{V}_{\mathcal{R}_i}
\label{eq:token_attention}
\end{equation}
其中，$\mathbf{Q}_i$、$\mathbf{K}_{\mathcal{R}_i}$、$\mathbf{V}_{\mathcal{R}_i}$ 分别表示第 $i$ 个窗口的查询矩阵和选定窗口的键值矩阵，$d_k$ 为键向量的维度。

\subsubsection{复杂度分析}

双层路由注意力机制通过 TopK 路由选择，显著降低了计算复杂度。标准自注意力机制的计算复杂度为 $O(n^2)$，其中 $n$ 为特征图中的像素数量。对于 16×16 的特征图，$n=256$，标准自注意力的计算复杂度为 $O(256^2) = O(65536)$。

双层路由注意力机制的计算复杂度为 $O(n^2 \times k/p^2)$，其中 $k$ 为 TopK 值（$k=4$），$p$ 为窗口大小（$p=4$）。对于 16×16 的特征图，划分为 16 个窗口，每个窗口选择 4 个相关窗口，计算复杂度为 $O(256^2 \times 4/16) = O(16384)$，相比标准自注意力降低了约 75\% 的计算量。

\subsubsection{模块作用小结}

TR 模块通过双层路由注意力机制实现三个核心功能：（1）建立特征图中任意位置之间的长距离依赖关系；（2）理解整个特征图的空间关系，更好地处理细长结构目标；（3）通过 TopK 路由选择降低计算复杂度约 75\%。TR 模块的输出特征经过 1×1 卷积降维后（16×16×512），与编码器的低层特征融合，为后续解码器提供增强的全局上下文信息。

\subsection{SAE空间细节增强模块}

针对 DeepLabV3+ 局部细节信息丢失的问题，本文在解码器特征融合之后引入 SAE（Spatial Attention Enhancement）空间细节增强模块。传统注意力机制（如 SE-Net\upcite{hu2019squeezeandexcitationnetworks}）仅关注通道维度，忽略空间位置信息；而单一的空间注意力机制可能无法充分利用通道间的依赖关系。因此，本文设计 SAE 模块结合坐标注意力和通道注意力，在空间和通道两个维度同时增强特征。

\subsubsection{坐标注意力机制}

坐标注意力（Coordinate Attention）通过分别在高度和宽度方向进行特征聚合，构建空间注意力权重\upcite{hou2021coordinateattentionefficientmobile}。

\textbf{H/W 方向池化}：对每个高度位置 $h$，在宽度维度上求平均；对每个宽度位置 $w$，在高度维度上求平均：
\begin{equation}
x_h[h, c] = \frac{1}{W} \sum_{w=1}^{W} x[h, w, c], \quad x_w[w, c] = \frac{1}{H} \sum_{h=1}^{H} x[h, w, c]
\label{eq:coord_hw}
\end{equation}
其中，$x_h$ 和 $x_w$ 分别为 H 和 W 方向的聚合特征，维度分别为 $[H, 1, C]$ 和 $[1, W, C]$。

\textbf{特征处理与权重生成}：将 H 和 W 方向的聚合特征拼接后，通过 1×1 卷积降维（降维比例 $r=4$），再通过分离卷积生成注意力权重：
\begin{equation}
a_h = \text{Sigmoid}(\text{Conv}_h(z)), \quad a_w = \text{Sigmoid}(\text{Conv}_w(z))
\label{eq:coord_weights}
\end{equation}
最终将权重应用于输入特征：$\mathbf{F}_{\text{coord}} = \mathbf{F} \odot a_h \odot a_w$，其中 $\odot$ 表示逐元素相乘。

\subsubsection{通道注意力机制}

通道注意力通过全局平均池化和多分支全连接层，生成通道权重以突出重要的特征通道。

\textbf{全局平均池化}：将空间维度压缩为 1，得到通道级特征：
\begin{equation}
z_c = \frac{1}{H \times W} \sum_{h=1}^{H} \sum_{w=1}^{W} x[h, w, c]
\label{eq:channel_pool}
\end{equation}

\textbf{多分支全连接层}：将通道特征通过 4 个并行的全连接层分支处理，每个分支输出 $C/4$ 维特征，拼接后得到 $C$ 维特征。这种多分支设计增强了特征的表达能力。

\textbf{通道权重生成}：通过全连接层生成通道注意力权重并应用：
\begin{equation}
a_c = \text{Sigmoid}(\text{FC}(z)), \quad \mathbf{F}_{\text{enhanced}} = \mathbf{F}_{\text{coord}} \odot a_c
\label{eq:sae_output}
\end{equation}
其中，$\mathbf{F}_{\text{enhanced}}$ 为 SAE 模块的最终输出特征。

\subsubsection{模块作用小结}

SAE 模块通过空间-通道双重注意力的协同增强，实现三个核心功能：（1）通过坐标注意力增强关键空间位置的特征响应；（2）通过通道注意力识别并突出对分割任务重要的特征通道；（3）在弱边界条件下提升边界分割精度。SAE 模块的输出特征（128×128×560）经过深度可分离卷积后，通过分类头输出像素级预测结果。

\subsection{混合损失函数}

OCT 图像语义分割任务面临严重的类别不平衡问题：背景区域通常占据图像的 90\% 以上，而锁孔目标区域仅占 5-10\%。传统交叉熵损失对每个像素赋予相同权重，导致模型训练时偏向背景类别。因此，本文设计混合损失函数，结合二元交叉熵损失和 Dice 损失，兼顾像素级精度和区域级一致性。

\subsubsection{二元交叉熵损失}

二元交叉熵损失（BCE Loss）提供逐像素的梯度信号，确保模型对每个像素进行准确的类别预测：
\begin{equation}
L_{\text{BCE}} = -\frac{1}{N} \sum_{i=1}^{N} \left[ y_i \log(\sigma(p_i)) + (1-y_i) \log(1-\sigma(p_i)) \right]
\label{eq:bce_loss}
\end{equation}
其中，$N$ 为像素总数，$y_i \in \{0, 1\}$ 为真实标签，$p_i$ 为预测 logits，$\sigma(\cdot)$ 为 Sigmoid 激活函数。

\subsubsection{Dice损失}

Dice 损失关注区域重叠，对类别不平衡问题更加鲁棒：
\begin{equation}
L_{\text{Dice}} = 1 - \frac{2 \sum_{i=1}^{N} \sigma(p_i) \cdot y_i + \epsilon}{\sum_{i=1}^{N} \sigma(p_i) + \sum_{i=1}^{N} y_i + \epsilon}
\label{eq:dice_loss}
\end{equation}
其中，$\epsilon = 10^{-5}$ 为平滑项。Dice 损失关注预测结果与真实标签的重叠程度，能够使模型更加关注前景区域的分割精度。

\subsubsection{组合损失函数}

将 BCE 损失和 Dice 损失进行加权组合：
\begin{equation}
L_{\text{total}} = \lambda_1 \cdot L_{\text{BCE}} + \lambda_2 \cdot L_{\text{Dice}}
\label{eq:total_loss}
\end{equation}
其中，$\lambda_1 = 2.0$ 和 $\lambda_2 = 2.0$ 分别为 BCE 损失和 Dice 损失的权重。BCE 损失提供逐像素的梯度信号，Dice 损失关注区域重叠，两者组合能够兼顾像素级精度和区域级一致性，从而提升分割性能。具体的训练策略（优化器、学习率调度、数据增强等）将在第四章实验部分详细介绍。

\subsection{本章小结}

本章详细阐述了本文提出的基于改进 DeepLabV3+ 的 OCT 图像语义分割方法。本章主要工作包括：

\begin{enumerate}
    \item 分析了 DeepLabV3+ 在 OCT 图像分割中的局限性：全局上下文建模不足和局部细节信息丢失。
    \item 设计了 TR 全局注意力模块：通过双层路由注意力机制建立长距离依赖，计算复杂度降低约 75\%。
    \item 设计了 SAE 空间细节增强模块：结合坐标注意力和通道注意力，增强边界细节表达。
    \item 设计了 BCE+Dice 混合损失函数：兼顾像素级精度和区域级一致性，应对类别不平衡问题。
\end{enumerate}

本文方法的主要创新点包括：

\begin{enumerate}
    \item \textbf{全局-局部双重注意力机制}：在编码器端增强全局上下文建模，在解码器端增强局部细节表达，两者协同提升分割性能。
    \item \textbf{端到端抗噪分割}：通过网络内部显式增强特征表达，使模型在散斑噪声干扰下稳定学习，减少对额外预处理的依赖。
    \item \textbf{高效计算设计}：TR 模块采用 TopK 路由降低计算复杂度，SAE 模块采用轻量级注意力设计，整体网络兼顾性能与效率。
\end{enumerate}

第四章将在自建数据集上对所提方法进行实验验证，包括对比实验、消融实验和可视化分析。