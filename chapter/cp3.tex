\clearpage

\section{基于改进DeepLabV3+的OCT图像语义分割方法}

\subsection{引言}

第二章介绍了 OCT 成像原理、深度学习基础以及语义分割的理论基础，为本章的方法设计提供了理论支撑。DeepLabV3+ 作为语义分割领域的经典方法，通过编码器-解码器结构和 ASPP 模块，在多尺度上下文信息捕获方面表现出色\upcite{Chen2018DeepLab}。然而，在处理激光焊接 OCT 图像时，DeepLabV3+ 仍面临以下挑战：

\textbf{（1）全局上下文信息利用不足}：ASPP 模块虽然通过多个不同空洞率的并行分支捕获多尺度上下文信息，但其感受野仍受限于空洞卷积的局部性，难以建立长距离依赖关系。在 OCT 图像中，锁孔区域往往呈现细长结构，需要网络理解整个图像的空间关系才能准确分割，而 ASPP 模块的局部感受野限制了其对全局结构的理解能力。

\textbf{（2）局部细节信息丢失}：DeepLabV3+ 的解码器通过融合高低层特征恢复空间分辨率，但在特征融合过程中，低层的细节信息可能被高层的语义信息所淹没，导致边界分割精度不足。在 OCT 图像中，锁孔边缘对比度低、边界模糊，需要网络保留更多的局部细节信息才能实现精确的边界定位。

针对上述问题，本文提出了一种基于改进 DeepLabV3+ 的 OCT 图像语义分割方法。该方法在编码器端引入 TR（Transformer Routing）全局注意力模块，通过双层路由注意力机制增强全局上下文建模能力；在解码器端引入 SAE（Spatial Attention Enhancement）空间细节增强模块，通过空间与通道注意力的协同增强，提升局部细节和边界信息的表征能力。此外，针对 OCT 图像中前景背景比例失衡的问题，本文设计了混合损失函数，结合交叉熵损失和 Dice 损失，平衡模型对不同类别的关注度。

本章将详细阐述本文提出的改进网络架构。3.2 节介绍整体网络框架和各模块的位置关系；3.3 节详细介绍 DeepLabV3+ 基线模型的设计及其局限性；3.4 节和 3.5 节分别阐述 TR 模块和 SAE 模块的设计原理与实现细节；3.6 节介绍混合损失函数的设计与训练策略；3.7 节对本章内容进行总结。

\subsection{总体框架}

本文提出的方法基于 DeepLabV3+ 的编码器-解码器架构，通过引入 TR 全局注意力模块和 SAE 空间细节增强模块，实现了对 OCT 图像中锁孔区域的高精度分割。整体网络架构如图 3-1 所示（如需要可添加架构图），主要包括编码器、ASPP 模块、TR 模块、解码器和 SAE 模块等核心组件。

\subsubsection{网络架构概述}

本文方法采用编码器-解码器结构，编码器负责提取多尺度特征并捕获语义信息，解码器负责恢复空间分辨率并输出像素级预测结果\upcite{Chen2018DeepLab}。具体而言，网络的数据流可描述为：输入图像经过编码器提取多尺度特征，编码器的高层特征经过 ASPP 模块进行多尺度上下文聚合，随后通过 TR 模块增强全局上下文建模能力，解码器融合高低层特征后通过 SAE 模块增强局部细节信息，最终通过分类头输出像素级类别预测结果。

\textbf{（1）编码器}：编码器采用 ResNet18-V1c 作为骨干网络\upcite{He2015ResNet}，通过四个阶段（Stage）的卷积和下采样操作，逐步提取从低级到高级的特征表示。编码器将输入图像（512×512×3）逐步下采样，最终在 Stage 4 输出高维语义特征（16×16×512）。编码器还保留了 Stage 1 的输出特征（128×128×64），用于后续解码器的特征融合。

\textbf{（2）ASPP 模块}：ASPP（Atrous Spatial Pyramid Pooling）模块位于编码器之后，通过多个不同空洞率的并行分支捕获多尺度上下文信息\upcite{Chen2017DeepLabV3}。ASPP 模块包含 5 个并行分支：1×1 卷积、dilation=12/24/36 的空洞卷积以及全局平均池化，这些分支的特征经过拼接后形成丰富的多尺度特征表示（16×16×2560）。

\textbf{（3）TR 模块}：TR（Transformer Routing）模块位于 ASPP 模块之后，通过双层路由注意力机制增强全局上下文建模能力。TR 模块将 ASPP 输出的特征图（16×16×2560）划分为多个窗口，通过区域级路由和 TopK 选择机制，仅对最相关的窗口进行注意力计算，在保持全局建模能力的同时降低了计算复杂度。TR 模块的输出特征（16×16×2560）经过 1×1 卷积降维后（16×16×512），通过上采样与编码器的低层特征进行融合。

\textbf{（4）解码器}：解码器通过上采样和特征融合操作，逐步恢复特征图的空间分辨率。解码器首先将 TR 模块输出的高层特征（16×16×512）上采样到 128×128，然后与编码器 Stage 1 的低层特征（128×128×64）进行融合。低层特征经过 1×1 卷积降维到 128×128×48，与高层特征拼接后得到融合特征（128×128×560）。

\textbf{（5）SAE 模块}：SAE（Spatial Attention Enhancement）模块位于解码器特征融合之后，通过坐标注意力和通道注意力的协同增强，提升局部细节和边界信息的表征能力。SAE 模块对融合特征（128×128×560）进行空间和通道维度的双重增强，输出增强后的特征（128×128×560）。

\textbf{（6）分类头}：分类头通过深度可分离卷积和 1×1 卷积，将增强后的特征转换为类别 logits（128×128×2），最后通过上采样得到与输入图像尺寸相同的分割结果（512×512×2）。

\subsubsection{特征图尺寸变化流程}

网络的特征图尺寸变化流程如下：

\begin{itemize}
    \item \textbf{输入}：512×512×3（RGB 图像）
    \item \textbf{编码器 Stage 1}：128×128×64（下采样到 1/4）
    \item \textbf{编码器 Stage 2}：64×64×128（下采样到 1/8）
    \item \textbf{编码器 Stage 3}：32×32×256（下采样到 1/16）
    \item \textbf{编码器 Stage 4}：16×16×512（下采样到 1/32）
    \item \textbf{ASPP 输出}：16×16×2560（5 个分支拼接）
    \item \textbf{TR 模块输出}：16×16×2560（注意力增强后）
    \item \textbf{TR 降维后}：16×16×512（1×1 卷积降维）
    \item \textbf{上采样后}：128×128×512（×8 上采样）
    \item \textbf{解码器融合}：128×128×560（高层特征 512 + 低层特征 48）
    \item \textbf{SAE 模块输出}：128×128×560（增强后）
    \item \textbf{分类头输出}：128×128×2（类别 logits）
    \item \textbf{最终输出}：512×512×2（×4 上采样）
\end{itemize}

上述特征图尺寸变化流程确保了网络能够在保持高分辨率细节信息的同时，充分利用深层的语义特征，从而实现精确的像素级分割。

\subsection{DeepLabV3+基线模型}

DeepLabV3+ 是语义分割领域的经典方法，通过编码器-解码器结构和 ASPP 模块，在多尺度上下文信息捕获方面表现出色\upcite{Chen2018DeepLab}。本节详细介绍 DeepLabV3+ 的架构设计，包括骨干网络、ASPP 模块和解码器，并分析其在处理 OCT 图像时的局限性，为后续改进模块的设计提供动机。

\subsubsection{编码器-解码器架构}

DeepLabV3+ 采用编码器-解码器架构，编码器负责提取多尺度特征并捕获语义信息，解码器负责恢复空间分辨率并输出像素级预测结果\upcite{Chen2018DeepLab}。编码器通常采用预训练的 CNN 骨干网络（如 ResNet、Xception 等），通过逐层卷积和下采样操作，将输入图像转换为高维特征表示；解码器通过上采样和特征融合操作，逐步恢复特征图的空间分辨率，并输出与输入图像尺寸相同的预测结果。

DeepLabV3+ 的创新之处在于将 DeepLabV3 的输出作为编码器特征，通过解码器融合编码器的中间特征，在保持分割精度的同时提升了边界定位的准确性\upcite{Chen2018DeepLab}。本文方法以 DeepLabV3+ 为基础框架，在此基础上引入 TR 模块和 SAE 模块，进一步提升分割性能。

\subsubsection{骨干网络：ResNet18-V1c}

本文方法采用 ResNet18-V1c 作为编码器的骨干网络\upcite{He2015ResNet}。ResNet18-V1c 是 ResNet 的改进版本，主要特点包括：

\textbf{（1）Deep Stem 结构}：ResNetV1c 使用三个连续的 3×3 卷积替换传统的 7×7 卷积，这种设计能够减少参数量并提升特征提取能力。Deep Stem 结构首先通过三个 3×3 卷积进行特征提取，然后通过最大池化操作进行下采样。

\textbf{（2）残差连接}：ResNet18-V1c 通过残差连接解决了深层网络的梯度消失问题，使得训练更深的网络成为可能\upcite{He2015ResNet}。每个残差块包含两个 3×3 卷积层，通过残差连接将输入直接传递到输出，简化了优化过程。

\textbf{（3）四个阶段的特征提取}：ResNet18-V1c 包含四个阶段（Stage），每个阶段通过卷积和下采样操作，逐步提取从低级到高级的特征表示：
\begin{itemize}
    \item \textbf{Stage 1}：输入 512×512×3，输出 128×128×64（下采样到 1/4）
    \item \textbf{Stage 2}：输入 128×128×64，输出 64×64×128（下采样到 1/8）
    \item \textbf{Stage 3}：输入 64×64×128，输出 32×32×256（下采样到 1/16）
    \item \textbf{Stage 4}：输入 32×32×256，输出 16×16×512（下采样到 1/32）
\end{itemize}

Stage 4 的输出特征（16×16×512）具有丰富的语义信息，被送入 ASPP 模块进行多尺度上下文聚合；Stage 1 的输出特征（128×128×64）保留了较多的细节信息，在解码器中与高层特征进行融合，用于恢复空间分辨率。

\subsubsection{ASPP模块设计}

ASPP（Atrous Spatial Pyramid Pooling）模块是 DeepLabV3+ 的核心组件，通过多个不同空洞率的并行分支捕获多尺度上下文信息\upcite{Chen2017DeepLabV3}。本文方法采用深度可分离 ASPP 模块（DepthwiseSeparableASPPModule），在保持性能的同时降低了计算复杂度。

ASPP 模块包含 5 个并行分支：

\textbf{（1）1×1 卷积分支}：使用标准 1×1 卷积，感受野为 1×1，用于捕获细节特征。该分支能够保留特征图的原始信息，避免空洞卷积可能带来的信息丢失。

\textbf{（2）3×3 空洞卷积分支（dilation=12）}：使用 3×3 空洞卷积，空洞率为 12，感受野约为 25×25，用于捕获局部特征。该分支能够在保持特征图分辨率的同时扩大感受野，捕获更大范围的上下文信息。

\textbf{（3）3×3 空洞卷积分支（dilation=24）}：使用 3×3 空洞卷积，空洞率为 24，感受野约为 49×49，用于捕获区域特征。该分支进一步扩大了感受野，能够捕获更大范围的上下文信息。

\textbf{（4）3×3 空洞卷积分支（dilation=36）}：使用 3×3 空洞卷积，空洞率为 36，感受野约为 73×73，用于捕获全局特征。该分支具有最大的感受野，能够捕获更大范围的上下文信息。

\textbf{（5）全局平均池化分支}：对特征图进行全局平均池化，然后通过 1×1 卷积和双线性插值上采样，恢复原始尺寸。该分支的感受野为整个特征图，能够捕获全局上下文信息。

5 个并行分支的输出特征经过拼接后，得到多尺度特征表示（16×16×2560）。随后通过 1×1 卷积降维到 16×16×512，减少特征维度并融合多尺度信息。

\textbf{深度可分离卷积}：ASPP 模块使用深度可分离卷积降低计算复杂度。深度可分离卷积分为两个步骤：首先进行深度卷积（Depthwise Convolution），对每个通道独立进行卷积操作；然后进行点卷积（Pointwise Convolution），使用 1×1 卷积进行通道融合。深度可分离卷积的参数量和计算量远小于标准卷积，在保持性能的同时提升了计算效率。

\subsubsection{解码器设计}

DeepLabV3+ 的解码器通过上采样和特征融合操作，逐步恢复特征图的空间分辨率\upcite{Chen2018DeepLab}。解码器的设计包括以下几个步骤：

\textbf{（1）高层特征上采样}：ASPP 模块输出的高层特征（16×16×512）经过 1×1 卷积降维后，通过双线性插值上采样到 128×128×512，恢复空间分辨率。

\textbf{（2）低层特征处理}：编码器 Stage 1 的低层特征（128×128×64）通过 1×1 卷积降维到 128×128×48，减少通道数以匹配高层特征的通道数。

\textbf{（3）特征融合}：将上采样后的高层特征（128×128×512）与处理后的低层特征（128×128×48）在通道维度进行拼接，得到融合特征（128×128×560）。这种设计能够结合低层的细节信息和高层的语义信息，提升分割精度。

\textbf{（4）分类头}：融合特征经过深度可分离卷积和 1×1 卷积，转换为类别 logits（128×128×2），最后通过双线性插值上采样到原始图像尺寸（512×512×2），输出像素级类别预测结果。

\subsubsection{DeepLabV3+的局限性分析}

尽管 DeepLabV3+ 在语义分割任务中表现出色，但在处理 OCT 图像时仍存在以下局限性：

\textbf{（1）全局上下文建模不足}：ASPP 模块虽然通过多个不同空洞率的并行分支捕获多尺度上下文信息，但其感受野仍受限于空洞卷积的局部性。在 OCT 图像中，锁孔区域往往呈现细长结构，需要网络理解整个图像的空间关系才能准确分割。ASPP 模块的局部感受野限制了其对全局结构的理解能力，可能导致细长结构的断裂或误分割。

\textbf{（2）局部细节信息丢失}：解码器通过融合高低层特征恢复空间分辨率，但在特征融合过程中，低层的细节信息可能被高层的语义信息所淹没。在 OCT 图像中，锁孔边缘对比度低、边界模糊，需要网络保留更多的局部细节信息才能实现精确的边界定位。DeepLabV3+ 的解码器设计相对简单，可能无法充分保留和利用低层的细节信息。

基于上述局限性分析，本文在编码器端引入 TR 模块以增强全局上下文建模能力，在解码器端引入 SAE 模块以增强局部细节和边界信息的表征能力，从而实现对锁孔区域的高精度分割。

\subsection{TR全局注意力模块}

针对 DeepLabV3+ 全局上下文建模不足的问题，本文在 ASPP 模块之后引入 TR（Transformer Routing）全局注意力模块，通过双层路由注意力机制增强全局上下文建模能力。TR 模块借鉴 BiFormer 的双层路由注意力思想\upcite{zhu2023biformervisiontransformerbilevel}，在保持全局建模能力的同时降低了计算复杂度，使得网络能够高效地建立长距离依赖关系。

\subsubsection{提出动机}

ASPP 模块虽然通过多个不同空洞率的并行分支捕获多尺度上下文信息，但其感受野仍受限于空洞卷积的局部性，难以建立长距离依赖关系。在 OCT 图像中，锁孔区域往往呈现细长结构，需要网络理解整个图像的空间关系才能准确分割。标准自注意力机制虽然能够建立长距离依赖关系，但其计算复杂度为 $O(n^2)$，对于高分辨率特征图而言计算开销过大\upcite{vaswani2017attentionneed}。

因此，本文引入 TR 模块，通过双层路由注意力机制，在保持全局建模能力的同时降低计算复杂度。TR 模块将特征图划分为多个窗口，通过区域级路由和 TopK 选择机制，仅对最相关的窗口进行注意力计算，从而在保持性能的同时提升了计算效率。

\subsubsection{模块设计}

TR 模块基于 BiFormer 的 BiformerBlock 设计\upcite{zhu2023biformervisiontransformerbilevel}，主要包括位置编码、LayerNorm 归一化、双层路由注意力、MLP 和残差连接等组件。

\textbf{（1）位置编码}：TR 模块使用深度可分离卷积（3×3，groups=dim）进行位置编码，为特征图添加位置信息。深度可分离卷积能够在不增加过多参数的情况下，为特征图添加空间位置编码。

\textbf{（2）LayerNorm 归一化}：在注意力计算之前，对输入特征进行 LayerNorm 归一化，稳定训练过程并提升模型性能。

\textbf{（3）双层路由注意力}：双层路由注意力是 TR 模块的核心组件，通过区域级路由和令牌级注意力两个阶段，实现高效的全局上下文建模。

\textbf{（4）MLP}：MLP 采用扩展-压缩结构，首先将特征维度扩展，然后压缩回原始维度，通过非线性变换增强特征表达能力。

\textbf{（5）残差连接}：TR 模块包含两个残差连接，分别位于注意力计算之后和 MLP 之后，使得网络能够学习残差映射，简化优化过程。

\subsubsection{双层路由注意力机制}

双层路由注意力机制是 TR 模块的核心创新，通过区域级路由和令牌级注意力两个阶段，实现高效的全局上下文建模。具体而言，双层路由注意力机制包括以下步骤：

\textbf{（1）窗口划分}：将输入特征图（16×16×2560）按照 4×4 的窗口大小进行划分，得到 16 个窗口（4×4=16）。每个窗口包含 16 个令牌（token），对应特征图中的 16 个像素位置。

\textbf{（2）区域级路由}：对每个窗口的所有令牌求平均，得到区域级查询向量 $\mathbf{Q}_{\text{region}}^{(i)}$ 和键向量 $\mathbf{K}_{\text{region}}^{(i)}$，其中 $i$ 表示第 $i$ 个窗口。区域级查询和键向量的计算可表示为：
\begin{equation}
\mathbf{Q}_{\text{region}}^{(i)} = \frac{1}{|\mathcal{W}_i|} \sum_{j \in \mathcal{W}_i} \mathbf{Q}_j, \quad \mathbf{K}_{\text{region}}^{(i)} = \frac{1}{|\mathcal{W}_i|} \sum_{j \in \mathcal{W}_i} \mathbf{K}_j
\label{eq:region_qk}
\end{equation}
其中，$\mathcal{W}_i$ 表示第 $i$ 个窗口内的所有令牌，$|\mathcal{W}_i|$ 表示窗口内的令牌数量。

计算窗口间的相似度矩阵 $\mathbf{S}$，其中 $\mathbf{S}_{ij}$ 表示第 $i$ 个窗口与第 $j$ 个窗口的相似度：
\begin{equation}
\mathbf{S}_{ij} = \frac{\mathbf{Q}_{\text{region}}^{(i)} \cdot \mathbf{K}_{\text{region}}^{(j)}}{\|\mathbf{Q}_{\text{region}}^{(i)}\| \times \|\mathbf{K}_{\text{region}}^{(j)}\|}
\label{eq:routing_similarity}
\end{equation}
其中，$\cdot$ 表示向量内积，$\|\cdot\|$ 表示向量范数。相似度矩阵 $\mathbf{S}$ 的维度为 16×16，表示 16 个窗口之间的相似度关系。

\textbf{（3）TopK 路由选择}：对每个窗口，根据相似度矩阵选择 TopK 个最相关的窗口参与注意力计算。本文设置 $k=4$，即每个窗口选择 4 个最相关的窗口。TopK 路由选择可表示为：
\begin{equation}
\mathcal{R}_i = \text{TopK}(\mathbf{S}_{i,:}, k=4)
\label{eq:topk_routing}
\end{equation}
其中，$\mathcal{R}_i$ 表示第 $i$ 个窗口选择的相关窗口索引集合，$\mathbf{S}_{i,:}$ 表示相似度矩阵的第 $i$ 行。

\textbf{（4）令牌级注意力}：在选定的窗口内进行像素级注意力计算。对于第 $i$ 个窗口，仅对 $\mathcal{R}_i$ 中的窗口进行注意力计算，从而降低计算复杂度。令牌级注意力可表示为：
\begin{equation}
\text{Attention}(\mathbf{Q}_i, \mathbf{K}_{\mathcal{R}_i}, \mathbf{V}_{\mathcal{R}_i}) = \text{softmax}\left(\frac{\mathbf{Q}_i \mathbf{K}_{\mathcal{R}_i}^T}{\sqrt{d_k}}\right) \mathbf{V}_{\mathcal{R}_i}
\label{eq:token_attention}
\end{equation}
其中，$\mathbf{Q}_i$、$\mathbf{K}_{\mathcal{R}_i}$、$\mathbf{V}_{\mathcal{R}_i}$ 分别表示第 $i$ 个窗口的查询矩阵和选定窗口的键值矩阵，$d_k$ 为键向量的维度。

\subsubsection{复杂度分析}

双层路由注意力机制通过 TopK 路由选择，显著降低了计算复杂度。标准自注意力机制的计算复杂度为 $O(n^2)$，其中 $n$ 为特征图中的像素数量。对于 16×16 的特征图，$n=256$，标准自注意力的计算复杂度为 $O(256^2) = O(65536)$。

双层路由注意力机制的计算复杂度为 $O(n^2 \times k/p^2)$，其中 $k$ 为 TopK 值（$k=4$），$p$ 为窗口大小（$p=4$）。对于 16×16 的特征图，划分为 16 个窗口，每个窗口选择 4 个相关窗口，计算复杂度为 $O(256^2 \times 4/16) = O(16384)$，相比标准自注意力降低了约 75\% 的计算量。

\subsubsection{模块作用}

TR 模块通过双层路由注意力机制，实现了以下功能：

\textbf{（1）捕获全局上下文信息}：通过区域级路由和令牌级注意力，TR 模块能够建立特征图中任意两个位置之间的依赖关系，捕获全局上下文信息。

\textbf{（2）理解整个特征图的空间关系}：通过窗口间相似度计算和 TopK 路由选择，TR 模块能够理解整个特征图的空间关系，从而更好地处理细长结构的目标。

\textbf{（3）降低计算复杂度}：通过 TopK 路由选择机制，TR 模块在保持全局建模能力的同时，显著降低了计算复杂度，提升了计算效率。

TR 模块的输出特征（16×16×2560）经过 1×1 卷积降维后（16×16×512），与编码器的低层特征进行融合，为后续解码器提供增强的全局上下文信息。

\subsection{SAE空间细节增强模块}

针对 DeepLabV3+ 局部细节信息丢失的问题，本文在解码器特征融合之后引入 SAE（Spatial Attention Enhancement）空间细节增强模块，通过坐标注意力和通道注意力的协同增强，提升局部细节和边界信息的表征能力。SAE 模块结合空间注意力与通道注意力，在空间和通道两个维度同时增强特征，从而提升边界分割精度。

\subsubsection{提出动机}

解码器通过融合高低层特征恢复空间分辨率，但在特征融合过程中，低层的细节信息可能被高层的语义信息所淹没，导致边界分割精度不足。在 OCT 图像中，锁孔边缘对比度低、边界模糊，需要网络保留更多的局部细节信息才能实现精确的边界定位。

传统的注意力机制（如 SE-Net 的通道注意力\upcite{hu2019squeezeandexcitationnetworks}）仅关注通道维度，忽略了空间位置信息；而空间注意力机制（如 CBAM 的空间注意力）仅关注空间维度，可能无法充分利用通道间的依赖关系。因此，本文设计 SAE 模块，结合坐标注意力和通道注意力，在空间和通道两个维度同时增强特征，从而提升边界分割精度。

\subsubsection{模块设计}

SAE 模块主要包括坐标注意力（CoordAtt）部分、卷积处理部分和通道注意力（SaELayer）部分。SAE 模块的输入为解码器融合后的特征（128×128×560），输出为增强后的特征（128×128×560）。

\textbf{（1）坐标注意力（CoordAtt）}：坐标注意力通过分别在高度和宽度方向进行特征聚合，构建空间注意力权重，增强关键空间位置\upcite{hou2021coordinateattentionefficientmobile}。

\textbf{H 方向池化}：对每个高度位置 $h$，在宽度维度 $W$ 上求平均，得到 H 方向的聚合特征：
\begin{equation}
x_h[h, c] = \frac{1}{W} \sum_{w=1}^{W} x[h, w, c]
\label{eq:coord_h}
\end{equation}
其中，$x[h, w, c]$ 表示输入特征在位置 $(h, w)$ 和通道 $c$ 的值，$x_h[h, c]$ 表示 H 方向聚合后的特征，输出维度为 $[H, 1, C]$。

\textbf{W 方向池化}：对每个宽度位置 $w$，在高度维度 $H$ 上求平均，得到 W 方向的聚合特征：
\begin{equation}
x_w[w, c] = \frac{1}{H} \sum_{h=1}^{H} x[h, w, c]
\label{eq:coord_w}
\end{equation}
其中，$x_w[w, c]$ 表示 W 方向聚合后的特征，输出维度为 $[1, W, C]$。

\textbf{特征拼接与降维}：将 H 方向和 W 方向的聚合特征进行拼接，得到 $[H+W, 1, C]$ 的特征。随后通过 1×1 卷积降维到 $[H+W, 1, C/r]$，其中 $r=4$ 为降维比例。

\textbf{分离卷积生成注意力权重}：通过分离卷积分别生成 H 和 W 方向的注意力权重：
\begin{equation}
a_h = \text{Sigmoid}(\text{Conv}_h(z)), \quad a_w = \text{Sigmoid}(\text{Conv}_w(z))
\label{eq:coord_weights}
\end{equation}
其中，$z$ 为降维后的特征，$\text{Conv}_h$ 和 $\text{Conv}_w$ 分别为 H 和 W 方向的分离卷积，$a_h$ 和 $a_w$ 分别为 H 和 W 方向的注意力权重。

\textbf{应用权重}：将注意力权重应用于输入特征：
\begin{equation}
\mathbf{F}_{\text{coord}} = \mathbf{F} \odot a_h \odot a_w
\label{eq:coord_apply}
\end{equation}
其中，$\odot$ 表示逐元素相乘，$\mathbf{F}$ 为输入特征，$\mathbf{F}_{\text{coord}}$ 为坐标注意力增强后的特征。

\textbf{（2）卷积处理}：坐标注意力增强后的特征经过两个连续的 3×3 卷积进行进一步处理，每个卷积后接 BatchNorm 和 ReLU 激活。卷积处理能够进一步融合空间信息，并通过残差连接保留原始特征信息。

\textbf{（3）通道注意力（SaELayer）}：通道注意力通过全局平均池化和多分支全连接层，生成通道注意力权重，突出重要的特征通道。

\textbf{全局平均池化}：对特征图进行全局平均池化，将空间维度压缩为 1，得到通道级特征：
\begin{equation}
z_c = \frac{1}{H \times W} \sum_{h=1}^{H} \sum_{w=1}^{W} x[h, w, c]
\label{eq:channel_pool}
\end{equation}
其中，$z_c$ 为通道 $c$ 的全局平均池化结果，输出维度为 $[C]$。

\textbf{4 分支全连接层处理}：将通道特征通过 4 个并行的全连接层分支进行处理，每个分支输出 $C/4$ 维特征，然后将 4 个分支的输出拼接，得到 $C$ 维特征。这种多分支设计能够增强特征的表达能力。

\textbf{通道权重生成与应用}：通过全连接层生成通道注意力权重：
\begin{equation}
a_c = \text{Sigmoid}(\text{FC}(z))
\label{eq:channel_weights}
\end{equation}
其中，$\text{FC}$ 表示全连接层，$a_c$ 为通道注意力权重。将通道注意力权重应用于特征：
\begin{equation}
\mathbf{F}_{\text{enhanced}} = \mathbf{F}_{\text{coord}} \odot a_c
\label{eq:sae_output}
\end{equation}
其中，$\mathbf{F}_{\text{enhanced}}$ 为 SAE 模块的最终输出特征。

\subsubsection{模块作用}

SAE 模块通过坐标注意力和通道注意力的协同增强，实现了以下功能：

\textbf{（1）增强空间位置信息}：坐标注意力通过 H 和 W 方向的聚合，能够捕获空间位置信息，增强关键空间位置的特征响应，从而提升边界分割精度。

\textbf{（2）增强重要通道}：通道注意力通过全局平均池化和多分支全连接层，能够识别对分割任务重要的特征通道，突出这些通道的特征响应。

\textbf{（3）提高边界分割精度}：通过空间和通道两个维度的双重增强，SAE 模块能够同时关注空间位置和通道重要性，从而提升边界分割精度，特别是在弱边界条件下。

SAE 模块的输出特征（128×128×560）经过深度可分离卷积降维后（128×128×512），通过分类头输出像素级类别预测结果。

\subsection{损失函数与训练策略}

语义分割任务通常面临类别不平衡问题，特别是在 OCT 图像中，背景区域占据图像的大部分，而锁孔目标区域相对较小。针对这一问题，本文设计了混合损失函数，结合交叉熵损失和 Dice 损失，平衡模型对不同类别的关注度。此外，本文还介绍了训练策略，包括优化器选择、学习率调度等关键超参数设置。

\subsubsection{问题分析}

在 OCT 图像语义分割任务中，前景（锁孔）和背景之间存在严重的类别不平衡问题。背景区域通常占据图像的 90\% 以上，而锁孔目标区域仅占图像的 5-10\%。这种类别不平衡会导致以下问题：

\textbf{（1）模型偏向背景类别}：由于背景像素数量远多于前景像素，模型可能倾向于将所有像素预测为背景，从而获得较高的像素准确率，但无法正确分割锁孔区域。

\textbf{（2）梯度信号不平衡}：传统的交叉熵损失对每个像素赋予相同的权重，导致背景像素的梯度信号远大于前景像素，使得模型难以学习前景特征。

\textbf{（3）评价指标误导}：在类别不平衡的情况下，像素准确率等指标可能产生误导性结果，无法准确反映模型的分割性能。

因此，需要设计能够处理类别不平衡问题的损失函数，使模型能够平衡学习前景和背景特征。

\subsubsection{混合损失函数设计}

本文采用二元交叉熵损失（Binary Cross Entropy Loss, BCE Loss）和 Dice 损失（Dice Loss）相结合的混合损失函数，兼顾像素级精度和区域级一致性。

\textbf{（1）二元交叉熵损失}：二元交叉熵损失提供逐像素的梯度信号，能够确保模型对每个像素进行准确的类别预测。对于二分类任务，BCE 损失可表示为：
\begin{equation}
L_{\text{BCE}} = -\frac{1}{N} \sum_{i=1}^{N} \left[ y_i \log(\sigma(p_i)) + (1-y_i) \log(1-\sigma(p_i)) \right]
\label{eq:bce_loss}
\end{equation}
其中，$N$ 为像素总数，$y_i \in \{0, 1\}$ 为真实标签（0 表示背景，1 表示前景），$p_i$ 为预测 logits，$\sigma(p_i) = 1/(1+e^{-p_i})$ 为 Sigmoid 激活后的概率。BCE 损失能够提供逐像素的梯度信号，确保模型对每个像素进行准确的类别预测。

\textbf{（2）Dice 损失}：Dice 损失关注区域重叠，对类别不平衡问题更加鲁棒。Dice 损失可表示为：
\begin{equation}
L_{\text{Dice}} = 1 - \frac{2 \sum_{i=1}^{N} \sigma(p_i) \cdot y_i + \epsilon}{\sum_{i=1}^{N} \sigma(p_i) + \sum_{i=1}^{N} y_i + \epsilon}
\label{eq:dice_loss}
\end{equation}
其中，$\epsilon = 10^{-5}$ 为平滑项，用于防止分母为零。Dice 损失关注预测结果与真实标签的重叠程度，对类别不平衡问题更加鲁棒，能够使模型更加关注前景区域的分割精度。

\textbf{（3）组合损失函数}：将 BCE 损失和 Dice 损失进行加权组合，得到最终的损失函数：
\begin{equation}
L_{\text{total}} = \lambda_1 \cdot L_{\text{BCE}} + \lambda_2 \cdot L_{\text{Dice}}
\label{eq:total_loss}
\end{equation}
其中，$\lambda_1 = 2.0$ 和 $\lambda_2 = 2.0$ 分别为 BCE 损失和 Dice 损失的权重。通过调整权重，可以平衡像素级精度和区域级一致性。BCE 损失提供逐像素的梯度信号，确保模型对每个像素进行准确的类别预测；Dice 损失关注区域重叠，对类别不平衡问题更加鲁棒。组合使用两种损失函数，能够兼顾像素级精度和区域级一致性，从而提升分割性能。

\subsubsection{训练策略}

本文采用以下训练策略进行模型训练：

\textbf{（1）优化器}：采用 Adam 优化器进行参数更新，Adam 优化器结合了动量和自适应学习率的优点，能够稳定训练过程并加速收敛。Adam 优化器的超参数设置为：$\beta_1 = 0.9$，$\beta_2 = 0.999$，权重衰减为 $5 \times 10^{-3}$。

\textbf{（2）学习率}：初始学习率设置为 $1 \times 10^{-6}$。较小的初始学习率能够确保训练过程的稳定性，特别是在使用预训练模型的情况下。

\textbf{（3）学习率调度}：采用多项式衰减（Polynomial Decay）策略进行学习率调度，学习率按迭代次数衰减：
\begin{equation}
\text{lr} = \text{lr}_{\text{base}} \times \left(1 - \frac{\text{iter}}{\text{max\_iter}}\right)^{\text{power}}
\label{eq:lr_schedule}
\end{equation}
其中，$\text{lr}_{\text{base}} = 1 \times 10^{-6}$ 为初始学习率，$\text{iter}$ 为当前迭代次数，$\text{max\_iter} = 20000$ 为最大迭代次数，$\text{power} = 0.9$ 为衰减指数。多项式衰减策略能够使学习率平滑下降，避免训练后期学习率过小导致的收敛缓慢。

\textbf{（4）批次大小}：批次大小设置为 8，较小的批次大小能够提供更多的梯度更新次数，有助于模型收敛。

\textbf{（5）迭代次数}：总迭代次数设置为 20,000 次，每 500 次迭代进行一次模型评估，每 10,000 次迭代保存一次模型检查点。

\textbf{（6）数据增强}：为提高模型泛化能力，训练时采用数据增强策略，包括随机翻转、光度失真（亮度、对比度、饱和度、色调调整）等。数据增强的详细设置将在第四章实验部分介绍。

通过上述训练策略，模型能够在类别不平衡的情况下稳定训练，并取得良好的分割性能。

\subsection{本章小结}

本章详细阐述了本文提出的基于改进 DeepLabV3+ 的 OCT 图像语义分割方法。本章的主要内容包括：

\textbf{（1）整体架构}：本文方法基于 DeepLabV3+ 的编码器-解码器结构，采用 ResNet18-V1c 作为编码器骨干网络，通过 ASPP 模块捕获多尺度上下文信息，在 ASPP 之后引入 TR 模块以增强全局依赖建模，在解码器特征融合之后引入 SAE 模块以增强局部细节与边界信息，最终通过分类头输出像素级类别预测结果。

\textbf{（2）TR 模块}：TR 模块通过双层路由注意力机制，在保持全局建模能力的同时降低了计算复杂度。TR 模块将特征图划分为多个窗口，通过区域级路由和 TopK 选择机制，仅对最相关的窗口进行注意力计算，从而在保持性能的同时提升了计算效率。TR 模块能够捕获全局上下文信息，理解整个特征图的空间关系，从而更好地处理细长结构的目标。

\textbf{（3）SAE 模块}：SAE 模块通过坐标注意力和通道注意力的协同增强，提升局部细节和边界信息的表征能力。坐标注意力通过 H 和 W 方向的聚合，能够捕获空间位置信息，增强关键空间位置的特征响应；通道注意力通过全局平均池化和多分支全连接层，能够识别对分割任务重要的特征通道。SAE 模块通过空间和通道两个维度的双重增强，提升了边界分割精度。

\textbf{（4）混合损失函数}：针对 OCT 图像中前景背景比例失衡的问题，本文设计了混合损失函数，结合交叉熵损失和 Dice 损失，平衡模型对不同类别的关注度。BCE 损失提供逐像素的梯度信号，确保模型对每个像素进行准确的类别预测；Dice 损失关注区域重叠，对类别不平衡问题更加鲁棒。组合使用两种损失函数，能够兼顾像素级精度和区域级一致性。

\textbf{（5）训练策略}：本文采用 Adam 优化器、多项式衰减学习率调度等训练策略，确保模型在类别不平衡的情况下稳定训练，并取得良好的分割性能。

本文方法的主要创新点包括：

\textbf{（1）双重注意力机制}：本文在编码器端引入 TR 模块以增强全局上下文建模，在解码器端引入 SAE 模块以增强局部细节表达，形成了全局-局部双重注意力机制，能够同时处理全局结构和局部细节，从而提升分割性能。

\textbf{（2）端到端的抗噪分割}：本文方法不将"去噪"作为独立处理步骤，而是将散斑与干扰视为成像条件的一部分，通过在分割网络内部显式增强全局上下文建模与局部边界细节表达，使模型在噪声干扰下仍能稳定学习锁孔的语义结构与轮廓，从流程上减少对额外预处理的依赖。

\textbf{（3）高效计算设计}：TR 模块通过双层路由注意力机制，在保持全局建模能力的同时降低了计算复杂度，相比标准自注意力减少了约 75\% 的计算量；SAE 模块采用轻量级的坐标注意力和通道注意力设计，计算开销较小。整体网络在保持高性能的同时，具有较高的计算效率。

第四章将在自建数据集上对所提方法进行全面的实验验证，包括与基线方法和主流分割网络的对比实验、消融实验验证各模块的有效性，以及可视化分析等，从而证明本文方法的有效性和优越性。